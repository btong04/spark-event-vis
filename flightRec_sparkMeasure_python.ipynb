{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Notebook for Testing sparkMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import sparkmeasure as spm\n",
    "\n",
    "# Initialize sparkMeasure metrics:\n",
    "stagemetrics = spm.StageMetrics(spark)\n",
    "taskmetrics = spm.TaskMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bytesRead: long (nullable = true)\n",
      " |-- bytesWritten: long (nullable = true)\n",
      " |-- completionTime: long (nullable = true)\n",
      " |-- diskBytesSpilled: long (nullable = true)\n",
      " |-- executorCpuTime: long (nullable = true)\n",
      " |-- executorDeserializeCpuTime: long (nullable = true)\n",
      " |-- executorDeserializeTime: long (nullable = true)\n",
      " |-- executorRunTime: long (nullable = true)\n",
      " |-- jobGroup: string (nullable = true)\n",
      " |-- jobId: long (nullable = true)\n",
      " |-- jvmGCTime: long (nullable = true)\n",
      " |-- memoryBytesSpilled: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- numTasks: long (nullable = true)\n",
      " |-- peakExecutionMemory: long (nullable = true)\n",
      " |-- recordsRead: long (nullable = true)\n",
      " |-- recordsWritten: long (nullable = true)\n",
      " |-- resultSerializationTime: long (nullable = true)\n",
      " |-- resultSize: long (nullable = true)\n",
      " |-- shuffleBytesWritten: long (nullable = true)\n",
      " |-- shuffleFetchWaitTime: long (nullable = true)\n",
      " |-- shuffleLocalBlocksFetched: long (nullable = true)\n",
      " |-- shuffleLocalBytesRead: long (nullable = true)\n",
      " |-- shuffleRecordsRead: long (nullable = true)\n",
      " |-- shuffleRecordsWritten: long (nullable = true)\n",
      " |-- shuffleRemoteBlocksFetched: long (nullable = true)\n",
      " |-- shuffleRemoteBytesRead: long (nullable = true)\n",
      " |-- shuffleRemoteBytesReadToDisk: long (nullable = true)\n",
      " |-- shuffleTotalBlocksFetched: long (nullable = true)\n",
      " |-- shuffleTotalBytesRead: long (nullable = true)\n",
      " |-- shuffleWriteTime: long (nullable = true)\n",
      " |-- stageDuration: long (nullable = true)\n",
      " |-- stageId: long (nullable = true)\n",
      " |-- submissionTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read metrics from json encoded flight recorder file:\n",
    "scope = 'stage' # Select 'task' or 'stage'\n",
    "\n",
    "load_perf_metrics = spark.read.option('inferSchema', True).json(r'/home/ubuntu/notebooks/tpcds/metrics/'+scope.lower()+'Metrics_rec_sample', multiLine=True)\n",
    "load_perf_metrics.createOrReplaceTempView('Perf'+scope.title()+'Metrics')\n",
    "load_perf_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, bytesRead: string, bytesWritten: string, completionTime: string, diskBytesSpilled: string, executorCpuTime: string, executorDeserializeCpuTime: string, executorDeserializeTime: string, executorRunTime: string, jobGroup: string, jobId: string, jvmGCTime: string, memoryBytesSpilled: string, name: string, numTasks: string, peakExecutionMemory: string, recordsRead: string, recordsWritten: string, resultSerializationTime: string, resultSize: string, shuffleBytesWritten: string, shuffleFetchWaitTime: string, shuffleLocalBlocksFetched: string, shuffleLocalBytesRead: string, shuffleRecordsRead: string, shuffleRecordsWritten: string, shuffleRemoteBlocksFetched: string, shuffleRemoteBytesRead: string, shuffleRemoteBytesReadToDisk: string, shuffleTotalBlocksFetched: string, shuffleTotalBytesRead: string, shuffleWriteTime: string, stageDuration: string, stageId: string, submissionTime: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to apply correct schema to fields. Otherwise aggregation will fail.\n",
    "load_perf_metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numStages': 0,\n",
       " 'numTasks': None,\n",
       " 'elapsedTime': None,\n",
       " 'stageDuration': None,\n",
       " 'executorRunTime': None,\n",
       " 'executorCpuTime': None,\n",
       " 'executorDeserializeTime': None,\n",
       " 'executorDeserializeCpuTime': None,\n",
       " 'resultSerializationTime': None,\n",
       " 'jvmGCTime': None,\n",
       " 'shuffleFetchWaitTime': None,\n",
       " 'shuffleWriteTime': None,\n",
       " 'resultSize': None,\n",
       " 'diskBytesSpilled': None,\n",
       " 'memoryBytesSpilled': None,\n",
       " 'peakExecutionMemory': None,\n",
       " 'recordsRead': None,\n",
       " 'bytesRead': None,\n",
       " 'recordsWritten': None,\n",
       " 'bytesWritten': None,\n",
       " 'shuffleRecordsRead': None,\n",
       " 'shuffleTotalBlocksFetched': None,\n",
       " 'shuffleLocalBlocksFetched': None,\n",
       " 'shuffleRemoteBlocksFetched': None,\n",
       " 'shuffleTotalBytesRead': None,\n",
       " 'shuffleLocalBytesRead': None,\n",
       " 'shuffleRemoteBytesRead': None,\n",
       " 'shuffleRemoteBytesReadToDisk': None,\n",
       " 'shuffleBytesWritten': None,\n",
       " 'shuffleRecordsWritten': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in flight recorder data. Register as spark table (e.g., \"PerfStageMetrics\", \"PerfTaskMetrics\").\n",
    "# Then apply aggretation methods. \n",
    "\n",
    "# Summarize metrics for entire application. Results same as stagemetrics.print_report().\n",
    "if scope.lower() == 'task':\n",
    "    aggregatedDF = taskmetrics.aggregate_taskmetrics_DF(\"PerfTaskMetrics\")\n",
    "elif scope.lower() == 'stage':\n",
    "    aggregatedDF = stagemetrics.aggregate_stagemetrics_DF(\"PerfStageMetrics\")\n",
    "\n",
    "# aggregatedDF cannot be converted to pandas dataframe: 'SparkSession' object has no attribute '_conf'\n",
    "# Work around using other operations. Same data as found in stagemetrics.print_report().\n",
    "perf_summary = dict(zip(aggregatedDF.columns, aggregatedDF.first()))\n",
    "perf_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+\n",
      "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|\n",
      "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+\n",
      "|        0|    null|       null|         null|           null|           null|                   null|                      null|                   null|     null|                null|            null|      null|            null|              null|               null|       null|     null|          null|        null|              null|                     null|                     null|                      null|                 null|                 null|                  null|                        null|               null|                 null|\n",
      "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregatedDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds38",
   "language": "python",
   "name": "ds38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
